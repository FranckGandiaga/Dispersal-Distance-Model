#############################################################################
#                                                                           #
#    Estimate SBW flight distance using Biosim simulations + Grainscape     #
#                                                                           #
#############################################################################

rm(list=ls())

################ PATHS - MAKE SURE THEY EXIST IN COMPUTER

biosimpath <- "path to folder for filtered data" #File with data after filtering
datapath <- "path to general data folder" # Most data stored
outputpath <- "path to distance loop output" # Output data of the loop
firpath <- "path to folder for merged fir data" # Merged data for Fir
sprucepath <-"path to folder for merged spruce data" # Merged data for Spruce
cleanedpath <- "path to folder for data ready for kernels" # Cleaned data ready for kernels
kernelpath <- "path to folder for Kernel figures" # Storage of final kernels
kerneloutput <- "path to folder for Kernel data output" # storage of kernel output


#Set directory
setwd(datapath)

#Load packages
library(grainscape)
library(fieldRS)
library(raster)
library(tidyverse)
library(igraph)
library(knitr)
library(ggplot2)
library(plyr)
library(data.table)
library(fdrtool)
library(survival)
library(stringr) # count characters in a row


#######################################
#                                     #
#     Module 1: Load Biosim data      #
#                                     #
#######################################

############ Data selection

# This module compiles the data generated by BioSIM and turns it into one usable file.

## Create adult moths presence files using Biosim full output across the area of interest.. 

# First we generated a map covering from Ontario to Nova Scotia with a 10*10km grid saved as an excel file.
# The map was used as base to run BioSIM
# Due to Excel number of rows limitations, BioSim simulations have to be run on a limited amount of raster cells. 
# In this study simulations are run from May 1st to October 31st from 2012 to 2014 and per increment of 5000 cells at a time.
# Due to Excel number of rows limitations, BioSim simulations have to be run on a limited amount of raster cells. 
# In this study simulations are run from May 1st to October 31st from 2012 to 2014 and per increment of 5000 cells at a time.

# BioSIM simulation outputs were saved as csv in segments in the "biosimpath" folder.

#Naming convention:
# For outputs using the Spruce phenological model finish files with "Spruce.csv"
# For outputs using the Fir phenological model finish files with "Fir.csv"


# WARNING: ALL MAPS SHOULD USE THE SAME COORDINATES SYSTEM
# For this study NAD_1983_Quebec_Lambert

# Set working directory

setwd(paste(biosimpath,sep=""))

#Spruce
tblSpruce <- list.files(pattern = "*Spruce.csv", full.names = TRUE) %>% 
  map_df(~read_csv(.x) %>%  mutate_all(as.numeric))
dataSpruce <- data.table(tblSpruce)
dataSpruce$date<-paste( dataSpruce$Year,"-", dataSpruce$Month,"-", dataSpruce$Day, sep="") 
write.table(dataSpruce,"CI100Spruce.txt")

#Fir
tblFir <- list.files(pattern = "*Fir.csv", full.names = TRUE) %>% 
  map_df(~read_csv(.x) %>% mutate_all(as.character))
dataFir <- data.table(tblFir)
dataFir$date<-paste( dataFir$Year,"-", dataFir$Month,"-", dataFir$Day, sep ="") 
write.table(dataFir,"CI100Fir.txt")

###################################################################################################
#                                                                                                 #
#   Module 2: Confidence Interval (CI) filtering - applied to BioSIM distribution for each site   #
#                                                                                                 #
###################################################################################################

#This module applies a confidence interval of your choice to your BioSIM data.


##### Sort data for MIN - CI at the start of the distribution - separates M1 samples (early migrants) from M2 (local moths)   ####

setwd(biosimpath) #File with data after filtering

#### Load data: ####
Fir = read.table("CI100Fir.txt",h=T)
Spruce = read.table("CI100Spruce.txt",h=T)

sites <- unique(Fir$Name) #same for Spruce, no need to change name
sites<-sites[!is.na(sites)] # remove NA generated by conversion into csv

#create table for saving:
# Spruce
CISpruce = data.frame(matrix(0, ncol = ncol(Spruce), nrow = 0))
colnames(CISpruce) <- colnames(Spruce)

##### Sort data for min ####
# SPRUCE
CI = 95 # can be a vector: c(66,95,99)
years <- c("2012","2013","2014")

for(z in 1:length(years)){
  year =years[z]
  datasp <- subset(Spruce, Year == years[z])
  # Subset each site
  
  
  for (m in 1: length(sites)){
    
    subsite <- subset(datasp, Name == sites[m])  
    #subsite <- subset(subsite, Adults != 0)
    
    for (l in 1: length(subsite$Adults) ) {
      subsite$sum [l] = sum(subsite$Adults[1:l])
    }
    
   
    min =c()
    #max=c()
    for(i in 1:length(CI)){
      min[i] = sum(subsite$Adults)*(((100-CI[i])/2)/100)
      #max[i] = sum(subsite$Adults)*((CI[i]+((100-CI[i])/2))/100) 
    }
    
    for (j in 1: length(subsite$Adults)){
      if(subsite$sum[j] < min[i]){
        j =j+1
      }else{
        subsite$Adults[1:j-1] <- 0 
        break
      }
    }
    CISpruce = rbind(CISpruce,subsite)
    print(paste("CI",CI,year,sites[m],sep="_"))
  }
  write.table(CISpruce,paste("CI",CI,year,"Spruce.txt",sep=""))
  CISpruce = data.frame(matrix(0, ncol = ncol(Spruce), nrow = 0))
  colnames(CISpruce) <- colnames(Spruce)
  }



# FIR
CI = 95 # can be a vector: c(66,95,99)
sites <- unique(Fir$Name) #same for Spruce, no need to change name
sites<-sites[!is.na(sites)] # remove NA generated by conversion into csv

#create table for saving:
# Fir
CIFir = data.frame(matrix(0, ncol = ncol(Fir), nrow = 0))
colnames(CIFir) <- colnames(Fir)

years <- c("2012","2013","2014")

for(z in 1:length(years)){
  year =years[z]
  datafr <- subset(Fir, Year == year)
  # Subset each site
  
  
  for (m in 1: length(sites)){
    
    subsite <- subset(datafr, Name == sites[m])  
    #subsite <- subset(subsite, Adults != 0)
    
    for (l in 1: length(subsite$Adults) ) {
      subsite$sum [l] = sum(subsite$Adults[1:l])
    }
    
   
    min =c()
    #max=c()
    for(i in 1:length(CI)){
      min[i] = sum(subsite$Adults)*(((100-CI[i])/2)/100)
      #max[i] = sum(subsite$Adults)*((CI[i]+((100-CI[i])/2))/100) 
    }
    
    for (j in 1: length(subsite$Adults)){
      if(subsite$sum[j] <= min[i]){
        j =j+1
      }else{
        subsite$Adults[1:j-1] <- 0 
        break
      }
    }
    CIFir = rbind(CIFir,subsite)
    print(paste("CI",CI,year,sites[m],sep="_"))
  }
  write.table(CIFir,paste("CI",CI,year,"Fir.txt",sep=""))
  CIFir = data.frame(matrix(0, ncol = ncol(Fir), nrow = 0))
  colnames(CIFir) <- colnames(Fir)
  }

####     Fuse outputs from first filter ####   
CI = 95 # can be a vector: c(66,95,99)
years <- c("2012","2013","2014")

#Fir
combinedFir = data.frame(matrix(0, ncol = 7, nrow = 0))
colnames(combinedFir) <- c("Name","Year","Month","Day","Adults","date","sum")

for (i in 1:length(years)){
  tempFir <- read.table(paste("CI",CI,years[i],"Fir.txt",sep=""),h=T)
  combinedFir <- rbind(combinedFir,tempFir)
}
write.table(combinedFir,paste("CI",CI,"Fir.txt",sep=""))

#Spruce
combinedSpruce = data.frame(matrix(0, ncol = 7, nrow = 0))
colnames(combinedSpruce) <- c("Name","Year","Month","Day","Adults","date","sum")

for (i in 1:length(years)){
  tempSpruce <- read.table(paste("CI",CI,years[i],"Spruce.txt",sep=""),h=T)
  combinedSpruce <- rbind(combinedSpruce,tempSpruce)
}
write.table(combinedSpruce,paste("CI",CI,"Spruce.txt",sep=""))



##### Sort data for MAX - CI at the start of the distribution - separates M3 samples (late migrants) from M2 (local moths)   ####

## Load first filtered data


# SPRUCE

CISpruce = data.frame(matrix(0, ncol = ncol(Spruce), nrow = 0))
colnames(CISpruce) <- colnames(Spruce)

#Load prefiltered data

CI = 95 # can be a vector: c(66,95,99)
Spruce <- read.table(paste("CI",CI,"Spruce.txt",sep=""), h=T)

sites <- unique(Spruce$Name) #same for Spruce, no need to change name


years <- c("2012","2013","2014")

for(z in 1:length(years)){
  year = years[z]
  datasp <- subset(Spruce, Year == years[z])
  # Subset each site
  
  
  for (m in 1: length(sites)){
    
    subsite <- subset(datasp, Name == sites[m])  
    #subsite <- subset(subsite, Adults != 0)
    
   # for (l in 1: length(subsite$Adults) ) {
  #    subsite$sum [l] = sum(subsite$Adults[1:l])
   # }
    
  
    #min =c()
    max=c()
    for(i in 1:length(CI)){
    #  min[i] = sum(subsite$Adults)*(((100-CI[i])/2)/100)
     max[i] = sum(subsite$Adults)*((CI[i]+((100-CI[i])/2))/100) 
    }
    
    for (j in 1: length(subsite$Adults)){
      if(subsite$sum[j] <= max[i]){
        j =j+1
      }else{
        subsite$Adults[j:nrow(subsite)] <- 0 
        break
      }
    }
    CISpruce = rbind(CISpruce,subsite)
    print(paste("Spruce_",year,sites[m],sep="_"))
    }
  write.table(CISpruce,paste("CI",CI,year,"Spruce_minmax.txt",sep=""))
  CISpruce = data.frame(matrix(0, ncol = ncol(Spruce), nrow = 0))
  colnames(CISpruce) <- colnames(Spruce)
  }

# FIR
Fir <- read.table(paste("CI",CI,"Fir.txt",sep=""), h=T)

CIFir = data.frame(matrix(0, ncol = ncol(Fir), nrow = 0))
colnames(CIFir) <- colnames(Fir)



sites <- unique(Fir$Name) #same for Fir, no need to change name


years <- c("2012","2013","2014")

for(z in 1:length(years)){
  year = years[z]
  datasp <- subset(Fir, Year == years[z])
  # Subset each site
  
  
  for (m in 1: length(sites)){
    
    subsite <- subset(datasp, Name == sites[m])  
    #subsite <- subset(subsite, Adults != 0)
    
    # for (l in 1: length(subsite$Adults) ) {
    #    subsite$sum [l] = sum(subsite$Adults[1:l])
    # }
    
  
    #min =c()
    max=c()
    for(i in 1:length(CI)){
      #  min[i] = sum(subsite$Adults)*(((100-CI[i])/2)/100)
      max[i] = sum(subsite$Adults)*((CI[i]+((100-CI[i])/2))/100) 
    }
    
    for (j in 1: length(subsite$Adults)){
      if(subsite$sum[j] <= max[i]){
        j =j+1
      }else{
        subsite$Adults[j:nrow(subsite)] <- 0 
        break
      }
    }
    CIFir = rbind(CIFir,subsite)
    print(paste("Fir_",year,sites[m],sep="_"))
  }
  write.table(CIFir,paste("CI",CI,year,"Fir_minmax.txt",sep=""))
  CIFir = data.frame(matrix(0, ncol = ncol(Fir), nrow = 0))
  colnames(CIFir) <- colnames(Fir)
}


####     Fuse outputs from filters ####    

## Load first filtered data
CI = 95 # can be a vector: c(66,95,99)
years <- c("2012","2013","2014")

#Fir
combinedFir = data.frame(matrix(0, ncol = 7, nrow = 0))
colnames(combinedFir) <- c("Name","Year","Month","Day","Adults","date","sum")

for (i in 1:length(years)){
  tempFir <- read.table(paste("CI",CI,years[i],"Fir_minmax.txt",sep=""),h=T)
  combinedFir <- rbind(combinedFir,tempFir)
}
write.table(combinedFir,paste("CI",CI,"Fir_minmax.txt",sep=""))

#Spruce
combinedSpruce = data.frame(matrix(0, ncol = 7, nrow = 0))
colnames(combinedSpruce) <- c("Name","Year","Month","Day","Adults","date","sum")

for (i in 1:length(years)){
  tempSpruce <- read.table(paste("CI",CI,years[i],"Spruce_minmax.txt",sep=""),h=T)
  combinedSpruce <- rbind(combinedSpruce,tempSpruce)
}
write.table(combinedSpruce,paste("CI",CI,"Spruce_minmax.txt",sep=""))


######################################################################
#                                                                    # 
#     module 3 : Load filtered/unfiltered data + Field sites data    #
#                                                                    # 
######################################################################

#Loads BioSIM data with the confidence interval generated in Module 2 as well as additional information on the field sites where adult moths were collected.


### Load BioSIM data with applied confidence interval

setwd(biosimpath) #File with data after filtering

# Choice of CI66 or CI95 0r CI 99

CI = 95

# Rename data to use in the loop
Spruce <- read.table(paste("CI",CI,"Spruce_minmax.txt",sep=""), h=T) # or add _minmax before .txt
Fir<- read.table(paste("CI",CI,"Fir_minmax.txt",sep=""), h=T) # or add _minmax before .txt
Spruce <- Spruce[,c(1,6,5)] #only keep name, date and adult rearranged in that order
Fir<- Fir[,c(1,6,5)] #only keep name, date and adult rearranged in that order 

#if "No suitable conditions for adult moths presence" keeps repeating
#easier without 0s
Spruce = subset(Spruce,Adults > 0 )
Fir = subset(Fir,Adults > 0 )

Spruce$date = format(as.Date(Spruce$date,"%Y-%m-%d"))
Fir$date = format(as.Date(Fir$date,"%Y-%m-%d"))


# Ready data for years effect analysis (paired with year defoliation)
Fir2012 = Fir[grep("2012", Fir$date), ]
Fir2013 = Fir[grep("2013", Fir$date), ]
Fir2014 = Fir[grep("2014", Fir$date), ]

Spruce2012 = Spruce[grep("2012", Spruce$date), ]
Spruce2013 = Spruce[grep("2013", Spruce$date), ]
Spruce2014 = Spruce[grep("2014", Spruce$date), ]


## Load specific sites data
# WARNING: ALL MAPS SHOULD USE THE SAME COORDINATES SYSTEM
# For this study NAD_1983_Quebec_Lambert

setwd(datapath)

#Load coordinnates of the cells used for BioSIM simulation
coord_or <- read.csv("BioSIM_sq100_ON_NFL.csv", h=T) 

#Load sample coordinnates
coord_samples <- read.csv("Trap locations_example.csv",sep =",", h=T)
sampling_dates <-  read.csv("Dates samples_example.csv",sep =",", h=T) 


#################################################################################################
#                                                                                               #
#    Module 4: Prepare any raster available to use as new base for geographical restriction     #
#                                                                                               #
#################################################################################################

# WARNING: ALL MAPS SHOULD USE THE SAME COORDINATES SYSTEM
# For this study NAD_1983_Quebec_Lambert

#This module is used to load additional maps to geographically restrict the area where dispersal is considered.
#For example in our study we looked at host species distribution as well as area of known defoliation during the Quebec outbreak.

### This code is built to accept any type of map for geographical restriction.

setwd(datapath)

##### Host Species

# Maps here were generated using the Beaudoin data.
# Rasters were converted in 10*10km raster and turned into csv table


#WARNING: ALL MAPS RastX and RastY VALUES MUST HAVE SAME PROJECTION: HERE NAD_1983_Quebec_Lambert

#As point tables
tablebfir <- read.csv("Balsam_fir_Beaudoin_2011.csv", h=T)
tablewspr <- read.csv("White_spruce_Beaudoin_2011.csv", h=T)
tablebspr <- read.csv("Black_spruce_Beaudoin_2011.csv", h=T)

### Select percentage of resource of interest
composition = 1


tablebfircomp <- subset(tablebfir,percentage >= composition)
tablewsprcomp <- subset(tablewspr,percentage >= composition)
tablebsprcomp <- subset(tablebspr,percentage >= composition)

#Fuse all files together using the cell id (all the same)
beaudFir2011 <- tablebfircomp
beaudSpr2011 <- tablewsprcomp
beaudoin2011 <- merge(tablebfircomp,tablewsprcomp, by =c("RastX","RastY"))
piceaspp2011 <- merge(tablebsprcomp,tablewsprcomp, by =c("RastX","RastY"))

nrow(mergedcomp)

##### Defoliation

# Maps here were generated using the defoliation data provided by the Canadian government.
# Rasters were converted in 10*10km raster and turned into csv table

defol2012 <- read.csv("Defoliation_2012_Coordinates.csv",h=T)
defol2013 <- read.csv("Defoliation_2013_Coordinates.csv",h=T)
defol2014 <- read.csv("Defoliation_2014_Coordinates.csv",h=T)

###################################################
#                                                 #
#     Module 5: Automated distances to origins    #
#                                                 #
###################################################

#This module uses all the data prepared and fed into it to automatically measure shortest distances between potential phenological origins and actual samples. 
#It automatically sorts through migrants and local individuals based on the phenological data provided by BioSIM and informs the user in real time.

setwd(datapath)

################# PREPARE ARGUMENTS BEFORE RUNNING THE LOOP #################

# select minimum percentage of adults as simulated by BioSIM- 0 = max range #currently folders for 0.1,0.5,1,2,3,4,5
min_adult = 0            
# BioSIM data selection Spruce or Fir or FirYEAR when testing for years
resource = Spruce

# Name for the files stored depending on the host species Fir - Spruce
resource_name = "Spruce"  

# minimum distance in meters for patch to be distinct from sample (preselection) - at least 7100 in case sample is in a corner
buff_prox <- 10000 

#list of sampling sites 
all_samples = sampling_dates[,c("Name")] #all_samples=all_samples[-1] # if need to run for some specific sites

# Option to restrict raster to a geographical area of interest or not by giving the output file a specific name.
# Either user wants no restriction then "NO RESTRICTION" has to match line 549: if(contained == "NO RESTRICTION"){  
# Any other option will restrict the map 
contained = "defol14_minmax"  # Possible options: "NO RESTRICTION" "Beaud2011_Fir+Spr_minmax",  "defol2014_minmax" 

# Specify map for geographical restrictions
# See Module 4 for map creation - in this study # piceaspp2011 (Fir+Spr) or #defol2014
contmap = beaudoin2011

# Confidence interval desired for the analysis - CI available have to match Module 3
CI = 95 

# Select package to create original patches. 
# "fieldRS" or "Grainscape". For Euclidean distances, fieldRS is sufficient and less buggy. In a future iteration Grainscape will allow for additional map options such as 'Least path resistance"
package = "fieldRS"

# Create empty object to save loop runs history
outputloop <- c()

################# DISTANCE MEASURING LOOP #################

for(k in 1:length(all_samples)){
  
  sample = all_samples[k]
  dates <- subset(sampling_dates, Name == sample)
  dates <- dates[ , colSums(is.na(dates)) == 0]
  vec_dates = as.vector(dates[,-1])
  vec_dates = as.character(vec_dates)
  dateselect = vec_dates    # Select date here or input "dates"
 
   #### Map creation and distance analysis ####
  
  dist_list <- vector(mode = "list", length = length(vec_dates))
  
  # Prepare data for loop
  for(j in 1:length(vec_dates)){
    data_date <- subset(resource, date == dateselect[j]) # put date of interest in "" Should match date of sample 
    coord_date_or <- merge(coord_or[,c("grid_code", "OBJECTID","Y","X")],data_date[,c("Name","Adults")], by.x="OBJECTID",by.y="Name") # link coordinnates with Biosim samples
    c_dat_or_pos <- subset(coord_date_or, Adults >min_adult) ## adjust what cell are selected by number of adults
    res<-raster("sq100_ON_to_NFL.tif") # 1 = QC, 2 = NS, 3 = NFL, 4 = NB, 5 = PEI, 6 = ON ,
    isBecomes <- cbind(c(1, 2, 3, 4,5,6), c(1,1,1,1,1,1)) # value at least = 1
    
  if(contained == "NO RESTRICTION"){  
    pts <- c_dat_or_pos[,c("X","Y")] # coordinates of cells with adults
  }else{
    # If want to adjust to Defoliated area
    #pts data
    pts <- c_dat_or_pos[,c("X","Y")] # coordinates of cells with adults
    rastpts  <- reclassify(res, rcl = isBecomes) # put values on raster
    rastpts[cellFromXY(rastpts, pts)] <- 2
    
    
    #defol data
    rastdefol  <- reclassify(res, rcl = isBecomes) #reload empty raster
    defol = contmap #table with defoliation 2014 data - must be on same projection system
    defolpts = defol[,c("RastX","RastY")] #only select coordinates
    rastdefol[cellFromXY(rastdefol, defolpts)] <- 2 #Set cells with defoliation to 2
    rastdefol[rastdefol==1] <-NA #only keep cells with defoliation
    #rastdefol[rastdefol==2] <- 1 
    
    # Create new raster
    rastmerged <- mask(rastpts, rastdefol) # only keep cells in common
    rastmerged[rastmerged==1] <-NA # only keep cells with adults
    mergedpts <- as.data.frame(rastmerged,xy = T) #turn raster in a table
    mergedpts <- na.omit(mergedpts) # only keep coordinates of adults
    pts <- mergedpts[,c(1,2)] # use new coordinates for rest of the loop
     }
    
    rast  <- reclassify(res, rcl = isBecomes)
    rast[cellFromXY(rast, pts)] <- 2
    coord <-subset(coord_samples[,c("Name", "X", "Y")], Name == sample ) #coordinates of sample
    Negpatches<- raster::extract(rast, coord[,c("X","Y")], buffer= buff_prox, fun = mean) # Adjust buffer for minimum distance from point
    
    # Run Loop 
    if(nrow(pts) <= 3 ){ # removes patches with low number of pixels to avoid hce error due to MPG function
      print(paste(sample,"_",dateselect[j],"_No suitable conditions for adult moths presence",sep=""))
      outputloop <- c(outputloop,paste(sample,"_",dateselect[j],"_No suitable conditions for adult moths presence",sep=""))    
          } else {
      if (Negpatches[1] > 1){
        print(paste(sample,"_",dateselect[j],"_Presence of moths expected - No analysis needed",sep=""))
        outputloop <- c(outputloop,paste(sample,"_",dateselect[j],"_Presence of moths expected - No analysis needed",sep=""))
        
                   } else {
        print(paste(sample,"_",dateselect[j],"_Presence of moths unexpected - Raster ready - Distances measured",sep=""))
        outputloop <- c(outputloop,paste(sample,"_",dateselect[j],"_Presence of moths unexpected - Raster ready - Distances measured",sep=""))
              
              if(package == "Grainscape"){
              #### Map with IDs
        patchyMPG <- MPG(rast, patch = (rast == 2))
        IDpatches <- ggGS(patchyMPG , "nodes")
        ptssample<- coord[,c(2,3)]
        rast_sple = rast
        rast_sple[cellFromXY(rast_sple, ptssample)] <- 2
        SpleMPG <- MPG(rast_sple, patch = (rast_sple == 2))
        #Show patches ID + overall info
        IDsple <- ggGS(SpleMPG , "nodes")
        IDsple <- IDsple[,c(2,6,7)]
        colnames(IDsple) <- c("Name","X","Y")
        # Plot with labels
        IDnames <- IDpatches[,c(2,6,7)]
        colnames(IDnames) <- c("Name","X","Y")
        Idtext <- rbind(coord,IDnames)
          #Distances
        cpoints <- ggGS(patchyMPG, "patchId") # coordinates of all pixels composing each patch
        colnames(cpoints) <- c("Name","X","Y") # have cpoints and "coord" match
        pot_origins <- unique(cpoints$Name)
              }else if(package == "fieldRS"){
                rast[rast==1] <-NA
                rastpatch <- ccLabel(rast, method = "simple", change.threshold = NULL)
                cpoints <- as.data.frame(rasterToPoints(rastpatch$regions,xy = T)) # coordinates of all pixels composing each patch
                colnames(cpoints) <- c("X","Y","Name") # have cpoints and "coord" match
                pot_origins <- unique(cpoints$Name)
              }else{
                print("Unknown package option")
              }
        dist_or <- matrix(c(rep(0, (length(pot_origins))*4)), ncol=4)
        for(i in 1:length(pot_origins)){ # loop to extract min distance from sample to patch
          or <- subset(cpoints, Name == pot_origins[i] ) # subset pixels of each patch
          if(nrow(or)< 5){ # line to select minimum amount of pixels
            
          }else{
            distmatrix <- rbind(coord,or) # coords of samples and pixels in the patch
            m <- dist(distmatrix) # distance between all points
            m <- m[1:nrow(or)] # only select distance from origin
            km <- m/1000 # convert distance in km
            table_km <- as.data.frame(km) # turn into table
            ord_tb_pts <- cbind(table_km,or)
            ord_table_km <- as.matrix(ord_tb_pts[order(ord_tb_pts[,1],decreasing = F),]) # put shortest distance first
            dist_or[i,] <- ord_table_km[1,] # select shortest distance 
            i = i+1
          }
        }
        dist_origins <- as.data.frame (dist_or) # turn vector into table
        #rownames(dist_origins) <- pot_origins # rename rows - patch names
        ord_dist_origins <- dist_origins[order(dist_origins[,1],decreasing = F),] # order to put shortest distance first
        ##colnames(ord_dist_origins) <- c(sample,"OldID","x","y") # name column as sample
        ord_dist_origins <- ord_dist_origins[rowSums(ord_dist_origins[,c(1:4)] == 0) == 0, ]
              }
          }
  
    if(nrow(pts) < 1 ){
      dist_list[[j]] <- rep("Unfavorable",length(vec_dates), ncol = length(vec_dates))
    }else{
      if (Negpatches[1] > 1){
        dist_list[[j]] <- rep("Expected",length(vec_dates), ncol = length(vec_dates))
      }else if (nrow(ord_dist_origins)==0){
        dist_list[[j]] <- rep("Unfavorable",length(vec_dates), ncol = length(vec_dates))
        }else{
        dist_list[[j]] <- ord_dist_origins[,1]
        #names(dist_list[[j]]) <- paste(sample,"_",colnames(dates[j]),sep="")
        j = j+1
      }
    }
  }
  disttbl <- data.frame(Reduce(cbind, dist_list))
  names(disttbl) <- paste(resource_name,"_",sample,"_",names(dates[-1]),sep="")
  rownames(disttbl) <- c(1:nrow(disttbl))
  # Set to save - create afile in the appropriate path with resource_name-space-min_adults
  dir.create(paste(outputpath,resource_name,"/",resource_name,"_",min_adult,"_CI",CI,"_rest-",contained,"_",package,sep="")) #automatically creates folder
  setwd(paste(outputpath,resource_name,"/",resource_name,"_",min_adult,"_CI",CI,"_rest-",contained,"_",package,sep="")) # set directory to that folder
  write.csv(disttbl,paste(resource_name,"_",min_adult,"_CI",CI,"_rest-",contained,"_Dist_origins_",sample,"_",package,".csv", sep="")) #save file
  #Reset directory after every loop
  setwd(datapath)
  k = k+1
}

# Save Loop run history as a .csv file
LoopMetaData <- as.data.frame(outputloop)
colnames(LoopMetaData) <- paste(resource_name,"_",min_adult,"_CI",CI,"_rest-",contained,"_Dist_origins_","_",package,".csv", sep="")
setwd(paste(outputpath,resource_name,"/",resource_name,"_",min_adult,"_CI",CI,"_rest-",contained,"_",package,sep="")) # set directory to that folder
write.csv(LoopMetaData, paste("DataLog_Dispersal_Distances_Loop_runs_",resource_name,"_",min_adult,"_CI",CI,"_rest-",contained,"_Dist_origins_","_",package,".csv", sep=""))

#Reset path
setwd(datapath)

######################################################
#                                                    #
#    Module 6: Compile data per period (M1 to M3)    #
#                                                    #
######################################################

#This module compiles all data generated in Module 5 and combines outputs form the Fir and Spruce models into one.

# PREPARE ARGUMENTS
#### Load all files to fuse them together do 1 file at the time

#specify the Confidence Interval desired
CI = 95 


# Select level of geographical restriction
contained = "defol14_minmax" # if want raster restricted to an area of interest - Other possibilities: "Beaud2011_Fir+Spr_minmax",  "defol2014_minmax" or "no_minmax". 

# Adjust minimum percentage of adults present
adlts = 0 #c(0.1,0.2,0.5,1,2,3,4)

# RUN CODE
resnames = c("Fir","Spruce")

for(x in 1:length(adlts)){
  adults =adlts [x]
    for(y in 1:2){
        if(resnames[y]=="Fir"){
          setwd(paste(firpath,adults,"_CI",CI,"_rest-",contained,"_",package,sep=""))
          #### Load all outputs from original Loop
          tbl <- list.files(pattern = "*.csv") %>% 
            map_df(~read_csv(.))

          
            # Create table without NA
    
            # step 1: coerce to data.table in place, move NAs to the bottom of each column, 
            # maintain the original order of non-NA values
            resultFir <- data.table(tbl)
            resultFir[resultFir=="Expected"] <- NA
            resultFir[resultFir=="Unfavorable"] <- NA
            resultFir <- resultFir[, lapply(.SD, function(x) x[order(is.na(x))])]
            resultFir <- resultFir[,!1] # remove first column - only a decount of the number of lines

            # Create a loop to determine maximum length of column with values in it
            collengthFir = c()

            for (i in 1:ncol(resultFir)) {
             collengthFir[i] = length(which(!is.na(resultFir[,..i])))
              i = i+1
              }


            # Remove rows with only NAs
            resultFir <- resultFir[1:max(collengthFir),]

            }else{
            # Do the same for Spruce
            #set directory
            setwd(paste(sprucepath,"_",adults,"_CI",CI,"_rest-",contained,"_",package,sep=""))

            tbl <- list.files(pattern = "*.csv") %>% 
            map_df(~read_csv(.))
            resultSpruce <- data.table(tbl)
            resultSpruce[resultSpruce=="Expected"] <- NA
            resultSpruce[resultSpruce=="Unfavorable"] <- NA
            resultSpruce <- resultSpruce[, lapply(.SD, function(x) x[order(is.na(x))])]
            resultSpruce <- resultSpruce[,!1] # remove first column - only a count of the number of lines

            collengthSpruce = c()
            for (i in 1:ncol(resultSpruce)) {
            collengthSpruce[i] = length(which(!is.na(resultSpruce[,..i])))
            i = i+1
            }
          resultSpruce <- resultSpruce[1:max(collengthSpruce),]
          }
        }

#Put both files together

maxnrow <- max(nrow(resultFir), nrow(resultSpruce))

# check which one is:
if(maxnrow - nrow(resultFir)!=0){
    #if Fir
    empty=matrix(c(rep.int(NA,length(resultFir))),nrow=maxnrow - nrow(resultFir),ncol=length(resultFir))  
    colnames(empty) = colnames(resultFir)  
    resultFir <- rbind(resultFir, empty)
  }else{
#if Spruce
empty=matrix(c(rep.int(NA,length(resultSpruce))),nrow=maxnrow - nrow(resultSpruce),ncol=length(resultSpruce))  
colnames(empty) = colnames(resultSpruce)  
resultSpruce <- rbind(resultSpruce, empty)
}



#Finish fusing
TotalData = cbind(resultFir,resultSpruce)
ncol(resultFir)
ncol(resultSpruce)
ncol(TotalData)

#Backup and save file
write.csv(TotalData, paste(cleanedpath,adults,"_CI",CI,"_rest-",contained,"_",package,".csv",sep=""))

#Remove the duplicates generated by the loop
TotalData = read.csv(paste(cleanedpath,adults,"_CI",CI,"_rest-",contained,"_",package,".csv",sep=""),h=T)
TotalData[is.na(TotalData)] <- 0
TotalData = TotalData[,-1]
#View(TotalData)

for(i in 2:nrow(TotalData) ){
  for(j in 1:ncol(TotalData)){
    if(TotalData[1,j] != 0){
      if(TotalData[i,j] == TotalData[1,j]){
        TotalData[c(i:nrow(TotalData)),j]= 0
      }else{
        TotalData[i,j] = TotalData[i,j]
      }
    }else{
    }
  }
}
TotalData[TotalData==0] <- NA

# Remove unnecessary NA

collength = c()
for (i in 1:ncol(TotalData)) {
  collength[i] = length(which(!is.na(TotalData[,i])))
  i = i+1
}
TotalData <- TotalData[1:max(collength),]

#Save file
write.csv(TotalData, paste(cleanedpath,adults,"_CI",CI,"_rest-",contained,"_cleaned","_",package,".csv",sep=""))

}



######################################################
#                                                    #
#   Module 7 : Dispersal kernels data preparation    #
#                                                    #
######################################################

#This module prepares the data by compiling same either by years or by category of events (migrants 1 and 3 or locals).

setwd(datapath)

# Minimal value of adults:

#CI = 95 #100,99,95,66
#contained = ""defol2014_minmax" # when changing no, change line 510 also # "Beaud2011_Fir+Spr_minmax",  "defol2014_minmax" or "no_minmax"
#package = "fieldRS" # "fieldRS" or "Grainscape"
#adlts = 0#c(0.1,0.2,0.5,1,2,3,4)
#for(z in 1:length(adlts)){

adults = 0 #adlts [z]
# per year
resource = c("Fir","Spruce") #Fir or Spruce
#either type
date = c("M1","M2","M3") 

datkerns <- read.csv(paste(cleanedpath,adults,"_CI",CI,"_rest-",contained,"_cleaned","_",package,".csv",sep=""))
data_kernels <- datkerns[1,]

#RESOURCE I
kern <- dplyr::select(data_kernels,contains(resource[1]))
kern <- kern[ , colSums(is.na(kern)) == 0]


# Check to see if all date values are present in kern
countnbrdate <- data.frame(number= 1:ncol(kern), string=colnames(kern),stringsAsFactors = F) # ready to check if date values present after distance loop
presdate <- as.data.frame(matrix(nrow = 1, ncol = length(date))) # prepare table to count nber of times one of the values was present 
colnames(presdate) <- date # rename columns with dates to match data
for(i in 1:length(date)){
  countnbrdate[,i+2] <- str_count(countnbrdate$string,date[i]) # check if date value is present in each column of the table
  presdate[i] <- sum(countnbrdate[i+2]) # count number of times the value was present
}
validdate <- presdate[, colSums(presdate != 0) > 0] # only keep columns where there was a values
revdate <- colnames(validdate) # only keep date values that were present in the loop
j <- rowSums(presdate!=0) # use j to generate new tables

# Create empty vectors to store data
distancesrd <- vector(mode = "list", length = j)
distancesnrd <- vector(mode = "list", length = j)

for( i in 1:length(date)){ 
  checkdate <- dplyr::select(kern,contains(date[i]))
  if(sum(checkdate) != 0){
    fk <-  dplyr::select(kern,contains(date[i]))
    fk <- t(fk)
    fk <- data.frame(fk)
    fk$rounded <- round_any(fk$X1,10)
    
    distancesrd[[i]] <- fk$rounded #or fk$X1 for non rounded
    distancesnrd[[i]] <- fk$X1 #or fk$X1 for non rounded
    
  }else{
    distancesrd[[i]] <- NA
    distancesnrd[[i]] <- NA
    
  }
  
}

#RESOURCE II
kern <- dplyr::select(data_kernels,contains(resource[2]))
kern <- kern[ , colSums(is.na(kern)) == 0]

# Check to see if all date values are present in kern
countnbrdate <- data.frame(number= 1:ncol(kern), string=colnames(kern),stringsAsFactors = F) # ready to check if date values present after distance loop
presdate <- as.data.frame(matrix(nrow = 1, ncol = length(date))) # prepare table to count nber of times one of the values was present 
colnames(presdate) <- date # rename columns with dates to match data
for(i in 1:length(date)){
  countnbrdate[,i+2] <- str_count(countnbrdate$string,date[i]) # check if date value is present in each column of the table
  presdate[i] <- sum(countnbrdate[i+2]) # count number of times the value was present
}
validdate <- presdate[, colSums(presdate != 0) > 0] # only keep columns where there was a values
revdate <- colnames(validdate) # only keep date values that were present in the loop
j <- rowSums(presdate!=0) # use j to generate new tables


for( i in 1:length(date)){
  checkdate <- dplyr::select(kern,contains(date[i]))
  if(sum(checkdate) != 0){
    fk <-  dplyr::select(kern,contains(date[i]))
    fk <- t(fk)
    fk <- data.frame(fk)
    fk$rounded <- round_any(fk$X1,10)
    
    distancesrd[[i+j]] <- fk$rounded #or fk$X1 for non rounded
    distancesnrd[[i+j]] <- fk$X1 #or fk$X1 for non rounded
    
  }else{
    distancesrd[[i+j]] <- NA
    distancesnrd[[i+j]] <- NA
  }
}

vect <- c()
for(a in 1:(length(date)+j)){
  vect[a] <- max(distancesrd[[a]])
}
vect<-vect[!is.na(vect)]
max.len <- max(vect)


for(i in 1:(j*2)){
  distancesrd[[i]] <- c(distancesrd[[i]], rep(NA, max.len - length(distancesrd[[i]])))
  distancesnrd[[i]] <- c(distancesnrd[[i]], rep(NA, max.len - length(distancesnrd[[i]])))
  i =i+1
}

distancerd_table <- data.frame(Reduce(cbind, distancesrd))
distancenrd_table <- data.frame(Reduce(cbind, distancesnrd))
distance_table <-cbind(distancerd_table,distancenrd_table)

name_dist <- c()

for(i in 1:j){
  name_dist[i] <- paste(resource[1],"_",date[i],sep="")
  name_dist[i+j] <- paste(resource[2],"_",date[i],sep="")
  name_dist[i+(j*2)] <- paste(resource[1],"_",date[i],sep="")
  name_dist[i+(j*3)] <- paste(resource[2],"_",date[i],sep="")
}


names(distance_table) <-name_dist



dist_roundedM <- distance_table
write.table(dist_roundedM,paste("CI",CI,"_dist_roundedM_rest-",contained,"_minmax_",package,".csv",sep=""))

#### or year
date = c("2012","2013","2014")

datkerns <- dplyr::select(datkerns,contains(c("M1","M2","M3"))) 

data_kernels <- datkerns[1,]

#RESOURCE I
kern <- dplyr::select(data_kernels,contains(resource[1]))
kern <- kern[ , colSums(is.na(kern)) == 0]


# Check to see if all date values are present in kern
countnbrdate <- data.frame(number= 1:ncol(kern), string=colnames(kern),stringsAsFactors = F) # ready to check if date values present after distance loop
presdate <- as.data.frame(matrix(nrow = 1, ncol = length(date))) # prepare table to count nber of times one of the values was present 
colnames(presdate) <- date # rename columns with dates to match data
for(i in 1:length(date)){
  countnbrdate[,i+2] <- str_count(countnbrdate$string,date[i]) # check if date value is present in each column of the table
  presdate[i] <- sum(countnbrdate[i+2]) # count number of times the value was present
}
validdate <- presdate[, colSums(presdate != 0) > 0] # only keep columns where there was a values
revdate <- colnames(validdate) # only keep date values that were present in the loop
j <- rowSums(presdate!=0) # use j to generate new tables

# Create empty vectors to store data
distancesrd <- vector(mode = "list", length = j)
distancesnrd <- vector(mode = "list", length = j)

for( i in 1:length(date)){ 
  checkdate <- dplyr::select(kern,contains(date[i]))
  if(sum(checkdate) != 0){
    fk <-  dplyr::select(kern,contains(date[i]))
    fk <- t(fk)
    fk <- data.frame(fk)
    fk$rounded <- round_any(fk$X1,10)
    
    distancesrd[[i]] <- fk$rounded #or fk$X1 for non rounded
    distancesnrd[[i]] <- fk$X1 #or fk$X1 for non rounded
    
  }else{
    distancesrd[[i]] <- NA
    distancesnrd[[i]] <- NA
    
  }
  
}

#RESOURCE II
kern <- dplyr::select(data_kernels,contains(resource[2]))
kern <- kern[ , colSums(is.na(kern)) == 0]

# Check to see if all date values are present in kern
countnbrdate <- data.frame(number= 1:ncol(kern), string=colnames(kern),stringsAsFactors = F) # ready to check if date values present after distance loop
presdate <- as.data.frame(matrix(nrow = 1, ncol = length(date))) # prepare table to count nber of times one of the values was present 
colnames(presdate) <- date # rename columns with dates to match data
for(i in 1:length(date)){
  countnbrdate[,i+2] <- str_count(countnbrdate$string,date[i]) # check if date value is present in each column of the table
  presdate[i] <- sum(countnbrdate[i+2]) # count number of times the value was present
}
validdate <- presdate[, colSums(presdate != 0) > 0] # only keep columns where there was a values
revdate <- colnames(validdate) # only keep date values that were present in the loop
j <- rowSums(presdate!=0) # use j to generate new tables


for( i in 1:length(date)){
  checkdate <- dplyr::select(kern,contains(date[i]))
  if(sum(checkdate) != 0){
    fk <-  dplyr::select(kern,contains(date[i]))
    fk <- t(fk)
    fk <- data.frame(fk)
    fk$rounded <- round_any(fk$X1,10)
    
    distancesrd[[i+j]] <- fk$rounded #or fk$X1 for non rounded
    distancesnrd[[i+j]] <- fk$X1 #or fk$X1 for non rounded
    
  }else{
    distancesrd[[i+j]] <- NA
    distancesnrd[[i+j]] <- NA
  }
}

vect <- c()
for(a in 1:(length(date)+j)){
  vect[a] <- max(distancesrd[[a]])
}
vect<-vect[!is.na(vect)]
max.len <- max(vect)


for(i in 1:(j*2)){
  distancesrd[[i]] <- c(distancesrd[[i]], rep(NA, max.len - length(distancesrd[[i]])))
  distancesnrd[[i]] <- c(distancesnrd[[i]], rep(NA, max.len - length(distancesnrd[[i]])))
  i =i+1
}

distancerd_table <- data.frame(Reduce(cbind, distancesrd))
distancenrd_table <- data.frame(Reduce(cbind, distancesnrd))
distance_table <-cbind(distancerd_table,distancenrd_table)

name_dist <- c()

for(i in 1:j){
  name_dist[i] <- paste(resource[1],"_",date[i],sep="")
  name_dist[i+j] <- paste(resource[2],"_",date[i],sep="")
  name_dist[i+(j*2)] <- paste(resource[1],"_",date[i],sep="")
  name_dist[i+(j*3)] <- paste(resource[2],"_",date[i],sep="")
}


names(distance_table) <-name_dist

dist_roundedY <- distance_table
write.table(dist_roundedY,paste("CI",CI,"_dist_roundedY_rest-",contained,"_minmax_",package,".csv",sep=""))




###########################################################
#                                                         #
#       Module 8: Prepare data for Kernel analysis        #
#                                                         #
###########################################################

#This module prepares the data in the format needed to run the dispersal kernel analysis in Module 9.

setwd(datapath)
CI = 95
contained = "defol14_minmax" # when changing no, change line 510 also # "Beaud2011_Fir+Spr_minmax",  "defol2014_minmax" or "no_minmax"
package = "fieldRS" # "fieldRS" or "Grainscape"
round.distances = "No" # "No" choose if you want to use rounded distances or not
types = "all" # "M1", "M2", "M3" # select type of migrant

#### CLosest patch distance (see Loop)
# Data comes from the "Script_automatic distances from origin"
dataY <- read.csv(paste("CI",CI,"_dist_roundedY_rest-",contained,"_minmax_",package,".csv",sep=""),sep=" ")
dataM <- read.csv(paste("CI",CI,"_dist_roundedM_rest-",contained,"_minmax_",package,".csv",sep=""),sep=" ")


#View(dataY)
#adjust length
#Put both files together
# add NA rows to shortest file
maxnrow <- max(nrow(dataY), nrow(dataM))

# check which one is:
#if dataY
if(maxnrow - nrow(dataY)!=0){
  empty=matrix(c(rep.int(NA,length(dataY))),nrow=maxnrow - nrow(dataY),ncol=length(dataY))  
  colnames(empty) = colnames(dataY)  
  dataY <- rbind(dataY, empty)
}else{
  #if dataM
  empty=matrix(c(rep.int(NA,length(dataM))),nrow=maxnrow - nrow(dataM),ncol=length(dataM))  
  colnames(empty) = colnames(dataM)  
  dataM <- rbind(dataM, empty)
}

# Fuse both
data <- cbind (dataY, dataM) # col 1 to 12 = years, 13 to 24 = sample time
#View(data)

#resource = c("Fir","Spruce") #Fir or Spruce
#date = c("M1","M2","M3") # 2012 to 2014 or can be replaced by "M1","M2","M3"

if(round.distances == "Yes"){
  selectdata <- dplyr::select(data,-contains(".1"))
}else{
  selectdata <- dplyr::select(data,contains(".1"))
}

#remove M2s
#selectdata <- dplyr::select(selectdata,-contains("M2"))

# all years fused 
 
dyFir <- dplyr::select(selectdata,-contains(c("Spruce")))
dyFir <- dplyr::select(dyFir,-contains(c("M")))
dyFir <- dyFir[!is.na(dyFir)]
write.csv(dyFir,paste("CI",CI,"_dyFir_",types,"_rest-",contained,"_",package,".csv",sep=""))

dySpruce <- dplyr::select(selectdata,-contains(c("Fir")))
dySpruce <- dplyr::select(dySpruce,-contains(c("M")))
dySpruce <- dySpruce[!is.na(dySpruce)]
write.csv(dySpruce,paste("CI",CI,"_dySpruce_",types,"_rest-",contained,"_",package,".csv",sep=""))

# Both resources fused
dyTot <- dplyr::select(selectdata,-contains("M"))
dyTot <- dyTot[!is.na(dyTot)]
write.csv(dyTot,paste("CI",CI,"_dyTot_",types,"_rest-",contained,"_",package,".csv",sep=""))

# Per type

if(types == c("all")){
  
  # all types fused
  dmFir <- dplyr::select(selectdata,contains(c("Fir")))
  dmFir <- dplyr::select(dmFir,contains(c("M")))
  dmFir <- dmFir[!is.na(dmFir)]
  write.csv(dmFir,paste("CI",CI,"_dmFir_",types,"_rest-",contained,"_",package,".csv",sep=""))
  
  dmSpruce <- dplyr::select(selectdata,contains(c("Spruce")))
  dmSpruce <- dplyr::select(dmSpruce,contains(c("M")))
  dmSpruce <- dmSpruce[!is.na(dmSpruce)]
  write.csv(dmSpruce,paste("CI",CI,"_dmSpruce_",types,"_rest-",contained,"_",package,".csv",sep=""))
  
  # Both resources fused
  dmTot <- dplyr::select(selectdata,contains("M"))
  dmTot <- dmTot[!is.na(dmTot)]
  write.csv(dmTot,paste("CI",CI,"_dmTot_",types,"_rest-",contained,"_",package,".csv",sep=""))
} else if(types== "M1"){
  # all types fused
  dmFir <- dplyr::select(selectdata,contains(c("Fir","M1")))
  dmFir <- dmFir[!is.na(dmFir)]
  write.csv(dmFir,paste("CI",CI,"_dmFir_",types,"_rest-",contained,"_",package,".csv",sep=""))
  
  dmSpruce <- dplyr::select(selectdata,contains(c("Spruce","M1")))
  dmSpruce <- dmSpruce[!is.na(dmSpruce)]
  write.csv(dmSpruce,paste("CI",CI,"_dmSpruce_",types,"_rest-",contained,"_",package,".csv",sep=""))
  
  # Both resources fused
  dmTot <- dplyr::select(selectdata,contains("M1"))
  dmTot <- dmTot[!is.na(dmTot)]
  write.csv(dmTot,paste("CI",CI,"_dmTot_",types,"_rest-",contained,"_",package,".csv",sep=""))
  
} else if(types== "M2"){
  # all types fused
  dmFir <- dplyr::select(selectdata,contains(c("Fir","M2")))
  dmFir <- dmFir[!is.na(dmFir)]
  write.csv(dmFir,paste("CI",CI,"_dmFir_",types,"_rest-",contained,"_",package,".csv",sep=""))
  
  dmSpruce <- dplyr::select(selectdata,contains(c("Spruce","M2")))
  dmSpruce <- dmSpruce[!is.na(dmSpruce)]
  write.csv(dmSpruce,paste("CI",CI,"_dmSpruce_",types,"_rest-",contained,"_",package,".csv",sep=""))
  
  # Both resources fused
  dmTot <- dplyr::select(selectdata,contains("M2"))
  dmTot <- dmTot[!is.na(dmTot)]
  write.csv(dmTot,paste("CI",CI,"_dmTot_",types,"_rest-",contained,"_",package,".csv",sep=""))
} else if(types == "M3"){
  # all types fused
  dmFir <- dplyr::select(selectdata,contains(c("Fir","M3")))
  dmFir <- dmFir[!is.na(dmFir)]
  write.csv(dmFir,paste("CI",CI,"_dmFir_",types,"_rest-",contained,"_",package,".csv",sep=""))
  
  dmSpruce <- dplyr::select(selectdata,contains(c("Spruce","M3")))
  dmSpruce <- dmSpruce[!is.na(dmSpruce)]
  write.csv(dmSpruce,paste("CI",CI,"_dmSpruce_",types,"_rest-",contained,"_",package,".csv",sep=""))
  
  # Both resources fused
  dmTot <- dplyr::select(selectdata,contains("M3"))
  dmTot <- dmTot[!is.na(dmTot)]
  write.csv(dmTot,paste("CI",CI,"_dmTot_",types,"_rest-",contained,"_",package,".csv",sep=""))
}

##########################################
#                                        #
#       Module 9: Kernel analysis        #
#                                        #
##########################################

#This module was expanded from the code developed by Hirsch et al. 2012 (Appendix S1). 
#It fits dispersal kernels distributions using Ordinary Least Squares regression (Hirsch et al. 2012; Appendix S1).
#It saves the data output of the analysis and creates the graphical representation of the analysis for a visual interpretation.

setwd(datapath)

#- ARGUMENTS TO SELECT DATA 
#Select confidence interval
CI = 95
#Select minimum percentage of adults
adults = 0 
# select package
package = "fieldRS" # "fieldRS" or "Grainscape"
#Select geographical restriction
contained="defol14_minmax"
#types of migrants
types = "all"# c("M1","M2","M3")# "M1", "M2", "M3", "M1", "M2", "M3"
#Compilation option for the data
comp_opt <- "Migrant" # Or "Year"
## For graphical cohesion
ticks = 12
#ticks = 1+(3.322*log(length(xtrunc)))

# Load data
if(comp_opt == "Year"){
  dyTot <- read.csv(paste("CI",CI,"_dyTot_",types,"_rest-",contained,"_",package,".csv",sep=""),h=T)
  dyTot <-dyTot[,-1]
  dyFir <- read.csv(paste("CI",CI,"_dyFir_",types,"_rest-",contained,"_",package,".csv",sep=""),h=T)
  dyFir <-dyFir[,-1]
  dySpruce <- read.csv(paste("CI",CI,"_dySpruce_",types,"_rest-",contained,"_",package,".csv",sep=""),h=T)
  dySpruce <-dySpruce[,-1]

}else{
dmTot <- read.csv(paste("CI",CI,"_dmTot_",types,"_rest-",contained,"_",package,".csv",sep=""),h=T)
dmTot <-dmTot[,-1]
dmFir <- read.csv(paste("CI",CI,"_dmFir_",types,"_rest-",contained,"_",package,".csv",sep=""),h=T)
dmFir <-dmFir[,-1]
dmSpruce <- read.csv(paste("CI",CI,"_dmSpruce_",types,"_rest-",contained,"_",package,".csv",sep=""),h=T)
dmSpruce <-dmSpruce[,-1]
}
###################################
#     Select data for kernels     #
###################################

datx = "dmSpruce" # Possible names for file and graph title - dyTot, dySpruce , dmSpruce, dyFir, dmFir or for all info not just 1st line kernSpA - kernFA 
xtrunc = dmSpruce
xhist = 900 # max value of x axis
colR <- "darkseagreen4" #"darkslategray3" # topo.colors(xtrunc) # Color of the histogram

# Prepare data for CTR 
CTRdata=data.frame(
  d=xtrunc,
  evnt=c(rep(1,length(xtrunc))))
# fitsurvival function 
CTR_function=survfit(Surv(CTRdata$d, event=CTRdata$evnt) ~ 1)
# return survival probabilties (P) corresponding to distances (D)
P=summary(CTR_function)$surv;D=summary(CTR_function)$time


######################## Define dispersal kernels ##############################
# these kernels are then fit through OLS (Ordinary Least Squares) to objects P & D

# log normal
SSLN=function(param){
  Ex=1-plnorm(D,meanlog=param[1],sdlog=param[2])
  sum((Ex-P)^2)
}

# Weibull
SSW=function(param){
  Ex=1-pweibull(D,shape=param[1],scale=param[2])
  sum((Ex-P)^2)
}

# exponential
SSEX=function(param){
  Ex=1-pexp(D,rate=param)
  sum((Ex-P)^2)
}

# Normal
SSN=function(param){
  Ex=1-phalfnorm(D,theta=param[1])
  sum((Ex-P)^2)
}



################ Obtain kernels with reconstructed tails #######################


#Fit each model to the censored data and store

fitLN=optim(c(1,1),SSLN)
LNpsave=c(fitLN$par[1],fitLN$par[2])

fitW=optim(c(1.2,55),SSW)
WBpsave=c(fitW$par[1],fitW$par[2])

# Note: above the OLS function was optimized with the Nelder-Mead algorithm 
# however this algorithm is optimal for optimization problems of 2 Dimensions
# or greater. The quasi-Newton method 'BFGS' is better suited for 1 D (or 1 
# parameter) problems. Alternatively the function 'optimize' can be used
# however result will be the same either way.

fitN=optim(c(0.05),SSN,method="BFGS")
Npsave=c(fitN$par[1])

fitEX=optim(c(0.01),SSEX,method="BFGS")
EXpsave=c(fitEX$par[1])

# choose best model based on AIC score

OLSscores=c(fitLN$value,fitW$value,
            fitN$value,fitEX$value)

# vector with number of parameters for each model
pars=c(2,2,1,1)
# calculating AIC from sum of squares
AICscores=(500*log(OLSscores/500) + 2*pars)

#################################### FINAL #####################################

#selecting bestfitting model
bestfit=c("LN","WB","N","EX")[which(AICscores==min(AICscores))]

# Save range values
rge = paste(CI,contained,datx,"range:",min(xtrunc),max(xtrunc),sep="_")
range_values = c(range_values,rge)

## Automatically saved as metafile

#win.metafile(paste(kernelpath,datx,"_CI",CI,"_M1M3_",bestfit,"_rest-",contained,"_",package,".wmf",sep=""))  

#build distribution histogram
hist(xtrunc,freq=FALSE,col = colR, breaks = ticks,main=paste(datx,CI,"_",contained,"_",bestfit,"_",package,sep=""),xlim=c(0,xhist),xlab="Distance (km)",
     ylim=c(0,0.008), ylab="Probability",axes=F)
axis(1, at = seq(0,xhist, 100))
axis(2, at = seq(0,0.008, 0.002))

#argument for curve
if(bestfit == "LN"){
  text(550,0.005,adj=0,bquote(mu == .(round(fitLN$par[1],2))))
  text(550,0.0045,adj=0,bquote(sigma == .(round(fitLN$par[2],2))))
}else if(bestfit == "WB"){
  text(550,0.005,adj=0,bquote(alpha == .(round(fitW$par[1],2))))
  text(550,0.0045,adj=0,bquote(beta == .(round(fitW$par[2],2))))
}else if(bestfit == "N"){
  text(550,0.005,adj=0,bquote(beta == .(round(fitW$par[1],2))))
  text(550,0.0045,adj=0,bquote(eta == .(round(fitW$par[2],2))))
}else if(bestfit == "EX"){
  text(550,0.005,adj=0,paste("y=",round(fitEX$par[1],4),"exp(",round(fitEX$value[1],4),"x)"))
  text(550,0.0045,adj=0,bquote(sigma == .(round(fitEX$value[1],2))))
}

#add curve
if (bestfit == "N"){
  curve(dhalfnorm(x,fitN$par[1],fitN$par[2]),0,xhist,col="black",lwd=2, add = T)
  
}else if(bestfit == "LN"){
  curve(dlnorm(x,fitLN$par[1],fitLN$par[2]),0,xhist,col="black",lwd=2, add = T)
  
}else if(bestfit == "WB"){
  curve(dweibull(x,fitW$par[1],fitW$par[2]),0,xhist,col="black",lwd=2, add = T)
  
}else if (bestfit == "EX"){
  curve(dexp(x,fitEX$par[1],fitEX$val[1]),0,xhist,col="black",lwd=2, add = T)
}

#If saving as metafile directly
#dev.off()

###############################
# Generate output files for final kernels
outxtrunc <- vector(mode = "list", length = length(adults)) #create vector to output raw data from curves
curvetype <- vector(mode = "list", length = length(adults)) #create vector to output name of type of curve
outpar1 <- vector(mode = "list", length = length(adults)) #create vector to output factor 1 of function
outpar2 <- vector(mode = "list", length = length(adults)) #create vector to output factor 2 of function

# generate nemes for that file
nameout <- c()

# Fill file
outxtrunc <- xtrunc
curvetype <- bestfit
outpar1 <- fitLN$par[1]
outpar2 <- fitLN$par[2]
nameout <- paste("All dates ", datx,adults,sep="")

#### Turn all data into tables for saving

xtrtbl <- data.frame(Reduce(rbind, outxtrunc))
names(xtrtbl) <- nameout
rownames(xtrtbl) <- 1:nrow(xtrtbl)
#rownames(disttbl) <- c(1:nrow(disttbl))
curvetbl <- data.frame(Reduce(cbind, curvetype))
par1tbl <- data.frame(Reduce(cbind, outpar1))
par2tbl <- data.frame(Reduce(cbind, outpar2))
names(curvetbl) <- nameout
names(par1tbl) <- nameout
names(par2tbl) <- nameout

FinalData <- rbind(curvetbl,par1tbl,par2tbl,"NA",xtrtbl)
write.csv(FinalData,paste(kerneloutput,"_",datx,"_",adults,"_CI",CI,"_",contained,"_",types,"_",package,".csv",sep=""))




#######################################################
#                                                     #
#    Module 10: Maps based on Biosim + Grainscape     #
#                                                     #
#######################################################

#This is the map generator module. It creates maps that are a visual representation of what the method does. 
#It measures and shows dispersal distances traveled from potential origins to a sample on the day of its collection. 
#The output is both a map and a table.
#By changing the different arguments this module can also create a map of the location of potential presence of adult moths based on their phenology.




rm(list=ls())

#Set directory
setwd(datapath)
#Packages


library(grainscape)
library(raster)
library(tidyverse)
library(igraph)
library(knitr)
library(rgeos)
library(fieldRS)



#Load coordinnates simulation
coord_or <- read.csv("sq100_ON_NFL.csv", h=T) # PEI and NFL included
## Can get only province if we want
#coord_or <- subset(coord_or, grid_code == "") # select grid_code linked to province #1 = QC, 2 = NS, 3 = NFL, 4 = NB, 5 = PEI, 6 = ON 


#Load smaple coordinnates
coord_samples <- read.csv("PstDc12-14_bis.csv",sep =",", h=T)

#Load sampling dates
sampling_dates <- read.csv("Dates samples.csv")

#################################
#     Generate overall map      #
#################################

setwd(biosimpath) #File with data after filtering

# Choice of CI66 or CI95 0r CI 99

CI = 95

# Rename data to use in the loop
Spruce <- read.table(paste("CI",CI,"Spruce_minmax.txt",sep=""), h=T) # or add _minmax before .txt
Fir<- read.table(paste("CI",CI,"Fir_minmax.txt",sep=""), h=T) # or add _minmax before .txt
Spruce <- Spruce[,c(1,6,5)] #only keep name, date and adult rearranged in that order
Fir<- Fir[,c(1,6,5)] #only keep name, date and adult rearranged in that order 

#if "No suitable conditions for adult moths presence" keeps repeating
#easier without 0s
Spruce = subset(Spruce,Adults > 0 )
Fir = subset(Fir,Adults > 0 )

Spruce$date = format(as.Date(Spruce$date,"%Y-%m-%d"))
Fir$date = format(as.Date(Fir$date,"%Y-%m-%d"))

setwd(datapath)

res<-raster("sq100_ON_to_NFL.tif") # 1 = QC, 2 = NS, 3 = NFL, 4 = NB, 5 = PEI, 6 = ON ,
#isBecomes <- cbind(c(1, 2, 3, 4,5,6), c(1,1,1,1,1,1)) # value at least = 1
#rast  <- reclassify(res, rcl = isBecomes) # put values on raster
plot(res,col="darkseagreen1", main = "Study area", legend = F)
plot(rasterToPolygons(res,dissolve=T), add=TRUE, border='black', lwd=1) # Adds border all around the map using package rgeos


#############################
#     Samples position      #
#############################


### Samples on map
# Sample sites
coord_samples <- read.csv("PstDc12-14_bis.csv",sep =",", h=T)
res<-raster("sq100_ON_to_NFL.tif") #1 = QC, 2 = NS, 3 = NFL, 4 = NB, 5 = PEI, 6 = ON ,
isBecomes <- cbind(c(1, 2, 3, 4,5,6), c(1,1,1,1,1,1)) # value at least = 1
patchPts  <- reclassify(res, rcl = isBecomes)
plot(patchPts,col=c("darkseagreen1"),main = "Sampling sites", legend=F)
points(coord_samples$X,coord_samples$Y,pch = 21, col = "black", bg = "red")
text(x = coord_samples$X, y = coord_samples$Y, label = coord_samples$Name, cex =0.5, pos =4)
plot(rasterToPolygons(res,dissolve=T), add=TRUE, border='black', lwd=1) # Adds border all around the map using package rgeos


#### Zoom option on map
cropbox1 <- drawExtent()
croprast <- crop(patchPts, cropbox1)
plot(croprast,col=c("darkseagreen1"), legend=F)
points(coord_samples$X,coord_samples$Y,pch = 21, col = "black", bg = "red")
text(x = coord_samples$X, y = coord_samples$Y, label = coord_samples$Name, cex =0.5, pos =4)
plot(rasterToPolygons(croprast,dissolve=T), add=TRUE, border='black', lwd=1) # Adds border all around the map using package rgeos




##########################
#     Generate maps      #
##########################

#### Date selection by hand ####

#Check sampling dates
#View(sampling_dates) 

# Have graph window in Full screen ready
dev.new()
par(mfrow=c(1,3)) # If want maps side by side

#### Arguments for Map creation and analysis ####

sample <- "T008"           # sample name
dateselect = "2013-07-08"    # Select date here - much match date format of resource object
min_adult = 0              # select minimum limit - 0 = max range
resource = Fir          # Spruce or Fir
resource_name = "Fir"   # Balsam fir - Spruce
buff_prox <- 7100          # minimum distance in meters for patch to be distinct from sample (preselection) - at least 7100 in case sample is in a corner
package = "fieldRS" #autre: Grainscape

# Map colors
colbase = "darkseagreen1" #color of the base layer of the map
colpatch = "darkgray" #color of potential origin patches
colsample = "red" #color of the sample point

# Additional map options
province_borders = "no" # add province borders to the map
patch_names = "no" # add patch names ordered from nearest to furthest from the sample
visible_distances = "no" # add segments representing euclidean distances
add_sample ="no" # add sample location - if no then only shows patches of phenological synchrony on that date

#### Map creation and distance analysis ####

# Prepare data for loop

data_date <- subset(resource, date == dateselect) # put date of interest in "" Should match date of sampledata_date <- subset(datefused, date =="L_Dates[i]") # put date of interest in "" 
coord_date_or <- merge(coord_or[,c("grid_code", "OBJECTID","Y","X")],data_date[,c("Name","Adults")], by.x="OBJECTID",by.y="Name") # link coordinnates with Biosim samples
c_dat_or_pos <- subset(coord_date_or, Adults >min_adult) ## adjust what cell are selected by number of adults
#c_dat_or_pos <- subset(c_dat_or_pos, grid_code == "") # subset per province # 1 = QC, 2 = NS, 3 = NFL, 4 = NB, 5 = PEI, 6 = ON
pts <- c_dat_or_pos[,c("X","Y")] # coordinates of cells with adults
res<-raster("sq100_ON_to_NFL.tif") # 1 = QC, 2 = NS, 3 = NFL, 4 = NB, 5 = PEI, 6 = ON ,
isBecomes <- cbind(c(1, 2, 3, 4,5,6), c(1,1,1,1,1,1)) # value at least = 1
rast  <- reclassify(res, rcl = isBecomes) # put values on raster
rast[cellFromXY(rast, pts)] <- 2 # pts= cell coordinates of adults
coord <-subset(coord_samples[,c("Name", "X", "Y")], Name == sample ) #coordinates of sample
Negpatches<- raster::extract(rast, coord[,c("X","Y")], buffer= buff_prox, fun = mean) # Adjust buffer for minimum distance from point


# Run Loop
if(nrow(pts) < 1 ){
  print("No suitable conditions for adult moths presence")
  plot(res,col=c(colbase), main = paste(resource_name," ",dateselect," ",min_adult,"%", sep=""), legend = F)
  points(coord$X,coord$Y,pch = 21, col = "black", bg = colsample,cex=2)
  #text(x = coord$X, y = coord$Y, label = coord$Name,pos = 4)
  ## Add Province borders
  plot(rasterToPolygons(res,dissolve=T), add=TRUE, border='black', lwd=1) # Adds border all around the map using package rgeos
  #or map border without provinces separation
  #isBecomes <- cbind(c(1, 2, 3, 4,5,6), c(1,1,1,1,1,1)) # value at least = 1
  #rast  <- reclassify(res, rcl = isBecomes) # put values on raster
  #plot(rasterToPolygons(rast,dissolve=T), add=TRUE, border='black', lwd=1) # Adds border all around the map using package rgeos
  
} else {
  if (Negpatches[1] > 1){
    print("Presence of moths expected - No analysis needed")
    #### Map with IDs
    patchyMPG <- MPG(rast, patch = (rast == 2))
    IDpatches <- ggGS(patchyMPG , "nodes")
    ptssample<- coord[,c(2,3)]
    rast_sple = rast
    rast_sple[cellFromXY(rast_sple, ptssample)] <- 2
    SpleMPG <- MPG(rast_sple, patch = (rast_sple == 2))
    #Show patches ID + overall info
    IDsple <- ggGS(SpleMPG , "nodes")
    colnames(IDsple) <- c("Name","X","Y")
    # Plot with labels
    IDnames <- IDpatches[,c(2,6,7)]
    colnames(IDnames) <- c("Name","X","Y")
    Idtext <- rbind(coord,IDnames)
    #Simple version
    plot(rast,col=c(colbase, colpatch), axes = F, legend = F,box = F)
    points(coord$X,coord$Y,pch = 21, col = "black", bg = colsample,cex=2)
    #text(x = coord$X, y = coord$Y, label = coord$Name,pos = 4) # name sample on map
    #points(Idtext$X,Idtext$Y,pch = 21, col = "black", bg = "black")
    #text(x = Idtext$X, y = Idtext$Y, label = Idtext$Name,pos = 4) # name patches on map
    #Distances
    ## Add Province borders
    plot(rasterToPolygons(res,dissolve=T), add=TRUE, border='black', lwd=1) # Adds border all around the map using package rgeos
    #or map border without provinces separation
    #isBecomes <- cbind(c(1, 2, 3, 4,5,6), c(1,1,1,1,1,1)) # value at least = 1
    #rast  <- reclassify(res, rcl = isBecomes) # put values on raster
    #plot(rasterToPolygons(rast,dissolve=T), add=TRUE, border='black', lwd=1) # Adds border all around the map using package rgeos
    
  } else {
    print("Presence of moths unexpected - Raster ready - Distances measured")
    if(package == "Grainscape"){
      #### Map with IDs
      patchyMPG <- MPG(rast, patch = (rast == 2))
      IDpatches <- ggGS(patchyMPG , "nodes")
      ptssample<- coord[,c(2,3)]
      rast_sple = rast
      rast_sple[cellFromXY(rast_sple, ptssample)] <- 2
      SpleMPG <- MPG(rast_sple, patch = (rast_sple == 2))
      #Show patches ID + overall info
      IDsple <- ggGS(SpleMPG , "nodes")
      IDsple <- IDsple[,c(2,6,7)]
      colnames(IDsple) <- c("Name","X","Y")
      # Plot with labels
      IDnames <- IDpatches[,c(2,6,7)]
      colnames(IDnames) <- c("Name","X","Y")
      Idtext <- rbind(coord,IDnames)
      #Distances
      cpoints <- ggGS(patchyMPG, "patchId") # coordinates of all pixels composing each patch
      colnames(cpoints) <- c("Name","X","Y") # have cpoints and "coord" match
      pot_origins <- unique(cpoints$Name)
    }else if(package == "fieldRS"){
      rastna <- rast #prepare new raster to replace 1 by NA in rast
      rastna[rastna==1] <-NA
      rastpatch <- ccLabel(rastna, method = "simple", change.threshold = NULL)
      cpoints <- as.data.frame(rasterToPoints(rastpatch$regions,xy = T)) # coordinates of all pixels composing each patch
      colnames(cpoints) <- c("X","Y","Name") # have cpoints and "coord" match
      pot_origins <- unique(cpoints$Name)
    }else{
      print("Unknown package option")
    }
    dist_or <- matrix(c(rep(0, (length(pot_origins))*4)), ncol=4)
    for(i in 1:length(pot_origins)){ # loop to extract min distance from sample to patch
      or <- subset(cpoints, Name == pot_origins[i] ) # subset pixels of each patch
      if(nrow(or)< 5){ # line to select minimum amount of pixels
        
      }else{
        distmatrix <- rbind(coord,or) # coords of samples and pixels in the patch
        m <- dist(distmatrix) # distance between all points
        m <- m[1:nrow(or)] # only select distance from origin
        km <- m/1000 # convert disatnce in km
        table_km <- as.data.frame(km) # turn into table
        ord_tb_pts <- cbind(table_km,or)
        ord_table_km <- as.matrix(ord_tb_pts[order(ord_tb_pts[,1],decreasing = F),]) # put shortest distance first
        dist_or[i,] <- ord_table_km[1,] # select shortest distance 
        i = i+1
      }
    }
    dist_origins <- as.data.frame (dist_or) # turn vector into table
    rownames(dist_origins) <- pot_origins # rename rows - patch names
    ord_dist_origins <- as.matrix(dist_origins[order(dist_origins[,1],decreasing = F),]) # order to put shortest distance first
    colnames(ord_dist_origins) <- c(sample,"x","y","PatchID") # name column as sample
    ord_dist_origins <- ord_dist_origins[rowSums(ord_dist_origins[,c(1:4)] == 0) == 0, ]
    ord_dist_origins <- as.data.frame(ord_dist_origins)
    ord_dist_origins$OrdID <- c(1:nrow(ord_dist_origins)) # Add column to rename patch in order of distance from short to long
    
    
    ## Plot base layer raster with patches
    # Version with details
    plot(rast,col=c(colbase, colpatch), main = paste(resource_name," ",dateselect," ",min_adult,"%", sep=""),axes=T, legend = F, box = T)
    # Version without details
    #plot(rast,col=c(colbase, colpatch),axes=F, legend = F, box = F)
    
    #Optional
    
    if(province_borders == "yes"){
      ## Add Province borders
      plot(rasterToPolygons(res,dissolve=T), add=TRUE, border='black', lwd=1) # Adds border all around the map using package rgeos
      #or map border without provinces separation
    }else{
      isBecomes <- cbind(c(1, 2, 3, 4,5,6), c(1,1,1,1,1,1)) # value at least = 1
      rast  <- reclassify(res, rcl = isBecomes) # put values on raster
      plot(rasterToPolygons(rast,dissolve=T), add=TRUE, border='black', lwd=1) # Adds border all around the map using package rgeos
    }
    ## Add legend
    legend("topright",
           bty = "n",
           legend = "SBW potential range",
           fill = colpatch,       # Color of the squares
           border = "black") # Color of the border of the squares
    if(add_sample=="yes"){
      # Add sample location
      points(coord$X,coord$Y,pch = 21, col = "black", bg = colsample,cex = 2)
      #Name sample
      text(x = coord$X, y = coord$Y, label = coord$Name,pos = 3) 
    }else{}
    #Name patches in order according to nearest distance (optional)
    if(patch_names == "yes"){
      text(x = ord_dist_origins$x, y = ord_dist_origins$y, label = ord_dist_origins$OrdID, cex = 1)
    }else{}
    # Add segments representing linear distances (optional)
    if(visible_distances == "yes"){
      segments(coord$X, coord$Y, ord_dist_origins$x, ord_dist_origins$y, col= "black",lty = "dashed")# ,lty = "dotted","dotdash","dashed" - Show distances on the map
    }else{}
    
    #### Summarise everything in a table
    # Show distances 
    #All info on the table
    #print(ord_dist_renamed)
    #without title
    sumtable <- ord_dist_origins[,c(5,1)]
    colnames(sumtable) <- c("OrdID", paste(dateselect,"_",sample,"_Dist(km)",sep=""))
    kable(sumtable,align ="r") 
    #with title
    kable(sumtable,col.names = c("Patch ID","Distance(km)"), caption = paste("Resource:",resource_name,", ","Date:",dateselect,", ","Adult percentage:",min_adult,"%",", ","Site:",sample, sep=""), align ="r") 
  }
}


     




